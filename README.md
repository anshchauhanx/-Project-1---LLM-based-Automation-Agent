# ğŸš€ LLM-Based Automation Agent

## ğŸ“Œ Overview
This project is an **LLM-powered automation agent** designed for **DataWorks Solutions**. It processes large volumes of logs, reports, and code artifacts and integrates with a **Continuous Integration (CI) pipeline**. The agent can interpret **plain-English tasks**, process data, and generate verifiable outputs.

## ğŸ› ï¸ Features
- âœ… **Task Execution via LLM**: Accepts plain-text instructions and executes tasks.
- âœ… **File Reading API**: Returns file contents for verification.
- âœ… **Operations Automation**: Handles tasks like formatting files, counting specific dates, and sorting JSON data.
- âœ… **Secure Data Handling**: Prevents unauthorized access and deletion.
- âœ… **Business Task Execution**: Fetch data from APIs, run SQL queries, transcribe audio, etc.
- âœ… **Docker Support**: Run the application in a **Docker container**.
- âœ… **IITM Proxy API Integration**: Uses **IITM's AI Proxy** for LLM-based processing.

---

## ğŸš€ Quick Start Guide

### 1ï¸âƒ£ **Clone the Repository**
```sh
git clone https://github.com/your-github-username/llm-automation-agent.git
cd llm-automation-agent
2ï¸âƒ£ Install Dependencies
Ensure you have Python 3.10+ installed, then run:

sh
Copy
Edit
pip install -r requirements.txt
3ï¸âƒ£ Set Up Environment Variables
Create a .env file in the root directory:

ini
Copy
Edit
AIPROXY_TOKEN=your_actual_proxy_token_here
4ï¸âƒ£ Run the FastAPI Server
sh
Copy
Edit
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
âœ… Server Running at: http://127.0.0.1:8000

ğŸ“¡ API Endpoints
1ï¸âƒ£ Execute a Task
Endpoint: POST /run?task=<task description>
Example Request:
sh
Copy
Edit
Invoke-RestMethod -Uri "http://127.0.0.1:8000/run?task=count%20Wednesdays%20in%20/data/dates.txt" -Method Post
Example Response:
json
Copy
Edit
{"message": "Task executed successfully", "output": "Wednesdays count: 3"}
2ï¸âƒ£ Read a File
Endpoint: GET /read?path=<file path>
Example Request:
sh
Copy
Edit
Invoke-RestMethod -Uri "http://127.0.0.1:8000/read?path=/data/dates-wednesdays.txt" -Method Get
Example Response:
json
Copy
Edit
{"content": "3"}
ğŸ³ Running with Docker
1ï¸âƒ£ Build the Docker Image
sh
Copy
Edit
docker build -t llm-agent .
2ï¸âƒ£ Run the Docker Container
sh
Copy
Edit
docker run -p 8000:8000 llm-agent
âœ… Now, access the API at: http://127.0.0.1:8000

ğŸ—ï¸ Project Structure
bash
Copy
Edit
llm-automation-agent/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI entry point
â”‚   â”œâ”€â”€ tasks.py         # Task execution logic
â”‚   â”œâ”€â”€ llm.py           # LLM processing (IITM Proxy API)
â”‚   â”œâ”€â”€ utils.py         # Utility functions
â”‚â”€â”€ data/                # Task-related input/output files
â”‚â”€â”€ requirements.txt     # Python dependencies
â”‚â”€â”€ Dockerfile           # Docker container setup
â”‚â”€â”€ .env                 # API keys & configurations
â”‚â”€â”€ README.md            # Project documentation
âš™ï¸ Configuration & Settings
The IITM Proxy API is used instead of OpenAI.
Modify .env file to configure API tokens.
Data files are stored in /data/ directory.
All outputs are written to /data/output/.
ğŸ› ï¸ Troubleshooting
ğŸ”´ Website Not Loading
Ensure FastAPI is running (uvicorn app.main:app --reload)
Try running on a different port (--port 8080)
Check for firewall or VPN restrictions
ğŸ”´ API Returning 400 Bad Request
Ensure IITM Proxy API Token is correct (.env file)
Test API manually:
sh
Copy
Edit
Invoke-RestMethod -Uri "https://aiproxy.sanand.workers.dev/openai/v1/chat/completions" -Method Post
Check FastAPI logs for errors (uvicorn app.main:app --log-level debug)
ğŸ¯ Future Improvements
âœ… Implement SQLite/DuckDB Support for database tasks
âœ… Add advanced error handling
âœ… Improve LLM task execution logic
ğŸ¤ Contributing
Fork the repository
Create a feature branch:
sh
Copy
Edit
git checkout -b feature-new-task
Commit changes & push to GitHub
Submit a Pull Request (PR)
ğŸ“œ License
This project is open-source under the MIT License.

ğŸŒŸ Acknowledgments
Developed for IITM TDS Project
Uses AI Proxy API by Sanand
Built with FastAPI & Python
ğŸš€ Ready to Automate Tasks? Let's Go!

yaml
Copy
Edit

---

### **ğŸš€ Next Steps**
âœ… **Save this as `README.md` in your GitHub repo**  
âœ… **Modify the `AIPROXY_TOKEN` section before publishing**  
âœ… **Push the project to GitHub using**:
```sh
git add README.md
git commit -m "Added project documentation"
git push origin main
